# Multimodal Trait & Emotion Recognition through Agentic AI Pipeline

A modular, **multiâ€‘agent conversational AI** pipeline that infers **emotions** and **Big Five personality traits** from interviewâ€‘style inputs enriched with behavioral metadata, then generates **empathic, traitâ€‘aware responses**. The system coordinates dedicated **Perception**, **Inference**, **Retrievalâ€‘Memory**, and **Dialogue** agents and supports multiple LLM backbones (e.g., **LLaMA 3.2â€‘1B**, **Falconâ€‘RWâ€‘1B**).

> Repository: `Sam-Titan/Multimodal-Trait-and-Emotion-Recognition-through-Agentic-AI-Pipeline`

---

## âœ¨ Key Features

- **Agentic workflow**: *Observe â†’ Reflect â†’ Act â†’ Selfâ€‘Audit*
- **Perception Agent** for emotion recognition from enriched text/metadata
- **Inference Agent** for Big Five (OCEAN) trait estimation
- **Retrievalâ€‘Augmented Memory** for contextual continuity across turns
- **Dialogue Agent** for personality/emotionâ€‘aware response generation
- **Multiâ€‘backbone** support (LLaMA 3.2â€‘1B, Falconâ€‘RWâ€‘1B)
- **Lightweight evaluation** via comparison utilities

---

## ğŸ—ï¸ Architecture (Highâ€‘Level)

```
User Input + Behavioral Metadata
        â”‚
        â–¼
  [Perception Agent] â”€â”€â–º Emotion labels
        â”‚
        â–¼
  [Inference Agent] â”€â”€â–º Big Five (OCEAN) scores
        â”‚
        â–¼
  [Retrieval Memory]  â”€â”€â–º Context from past interactions (RAG)
        â”‚
        â–¼
  [Dialogue Agent]    â”€â”€â–º Empathic, traitâ€‘aware response
```

---

## ğŸ“ Repository Layout (topâ€‘level)

```
Multimodal-Trait-and-Emotion-Recognition-through-Agentic-AI-Pipeline/
â”œâ”€ Llama_dialogue_agent.py         # Dialogue agent using LLaMA 3.2â€‘1B
â”œâ”€ Falcon_dialogue_agent.py        # Dialogue agent using Falconâ€‘RWâ€‘1B
â”œâ”€ compare_models.py               # Simple comparison / benchmarking utility
â”œâ”€ aligned_data_balanced.json      # Sample aligned data (balanced)
â”œâ”€ aligned_data_with_traits.json   # Sample aligned data with trait labels
â”œâ”€ llama_aligned_traits.json       # LLaMAâ€‘specific aligned trait examples
â”œâ”€ falcon_aligned_traits.json      # Falconâ€‘specific aligned trait examples
â””â”€ README.md
```

> Languages: **Python**

---

## âš™ï¸ Setup

### 1) Clone
```bash
git clone https://github.com/Sam-Titan/Multimodal-Trait-and-Emotion-Recognition-through-Agentic-AI-Pipeline.git
cd Multimodal-Trait-and-Emotion-Recognition-through-Agentic-AI-Pipeline
```

### 2) Environment
Create a virtual environment (Python â‰¥3.10 recommended):
```bash
# Windows (PowerShell)
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# macOS/Linux
python3 -m venv .venv
source .venv/bin/activate
```

### 3) Install dependencies
If a `requirements.txt` exists:
```bash
pip install -r requirements.txt
```
If not, a minimal stack for local experiments usually includes:
```bash
pip install transformers accelerate datasets sentencepiece
# add: tiktoken, langchain, faiss-cpu, numpy, pandas as needed
```

> Tip: For HF models that require gated access or large downloads, prefer GPU machines and set `HF_HOME` to a disk with sufficient space.

---

## ğŸš€ Quickstart

### A) Run the LLaMAâ€‘based dialogue agent
```bash
python Llama_dialogue_agent.py
```
### B) Run the Falconâ€‘based dialogue agent
```bash
python Falcon_dialogue_agent.py
```
### C) Compare backbones
```bash
python compare_models.py
```

> Note: These scripts are intended as starting points. Open the files and adjust model IDs, prompts, and evaluation settings to your environment (GPU/CPU, quantization, context length, etc.).

---

## ğŸ§© Data

The repository ships a few **aligned JSON** files for quick tests and demos:
- `aligned_data_balanced.json` â€“ balanced examples across labels
- `aligned_data_with_traits.json` â€“ examples enriched with OCEAN traits
- `llama_aligned_traits.json`, `falcon_aligned_traits.json` â€“ backboneâ€‘specific aligned sets

You can extend the schema with additional metadata (e.g., *response latency*, *body language flags*, *speech prosody bins*) to improve the Perception/Inference stages.

---

## ğŸ”§ Configuration Ideas

- **Backbones**: swap HF model IDs or add quantized variants (e.g., 4â€‘bit)
- **RAG**: wire a vector store (FAISS/Chroma) for retrieval memory
- **Prompting**: add system prompts for *selfâ€‘audit* and *bias checks*
- **Scoring**: log empathy/latency/diversity metrics during comparisons
- **Safety**: add refusal/grounding rules for risky or medical claims

---

## ğŸ§ª Evaluation (lightweight)

- **Backbone comparison** using `compare_models.py`
- Track **latency**, **response diversity**, **empathy heuristics**
- Optionally add external human ratings or pairwise preference voting

---

## ğŸ“¦ Deployment Notes

- Set environment variables (e.g., model IDs, API keys if any)
- Prefer GPU runners; enable `torch.compile()` or `accelerate`
- Containerize with a minimal `Dockerfile` and `CMD` to run your chosen agent

---

## ğŸ—ºï¸ Roadmap

- Add `requirements.txt` and CLI flags
- Integrate a vector DB for Retrieval Memory
- Extend Perception to multimodal I/O (ASR, vision embeddings)
- Add JSON logging + eval dashboards
- Basic unit tests and CI

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch: `git checkout -b feat/my-feature`
3. Commit: `git commit -m "feat: add my feature"`
4. Push: `git push origin feat/my-feature`
5. Open a Pull Request

---

## ğŸ“œ License

If no license file is present, treat the code as **All Rights Reserved** by the author. Please open an issue to discuss reuse/distribution.

---

## ğŸ™Œ Acknowledgements

Built around compact open LLMs (e.g., **LLaMA 3.2â€‘1B**, **Falconâ€‘RWâ€‘1B**) with an **agentic design** (Perception â†’ Inference â†’ Retrieval Memory â†’ Dialogue). Inspired by modern RAG and multiâ€‘agent orchestration patterns.
